# During development, please create a `data` directory in the root directory of your project, 
# then create an empty file named `.config.yaml` in the `data` directory.
# If you want to change or override any configuration, modify the `.config.yaml` file rather than changing the `config.yaml` file.
# The system will give priority to loading the configuration from `data/.config.yaml`. If a configuration item is not found in `.config.yaml`, 
# the system will automatically look it up in `config.yaml`.
# This approach simplifies configuration and protects the security of your keys.
# If you are using Zhikonsole, then all the configuration below will not take effect; please modify your configuration in Zhikonsole.

# #####################################################################################
# #############################The following are basic server runtime configurations####################################
server:
  # Server listening address and port
  ip: 0.0.0.0
  port: 8000
  # Port for the HTTP service, used for the simple OTA interface (single-service deployment) and vision analysis interface
  http_port: 8003
  # This websocket configuration is the websocket address that the ota interface sends to the device.
  # By default, the ota interface will automatically generate a websocket address and output it to the startup log.
  # You can directly use your browser to access the ota interface for confirmation.
  # When deploying with Docker or public network (using SSL, domain name), this address may not be accurate.
  # Therefore, if you use Docker deployment, set websocket to LAN address.
  # If you use public network deployment, set vwebsocket to a public address.
  websocket: ws://your_ip_or_domain:port/xiaozhi/v1/
  # Visual analysis interface address
  # The interface address for sending vision analysis to the device
  # If using the default setting below, the system will generate the vision recognition address automatically and output it to the startup log.
  # You can directly use your browser to access this address for confirmation.
  # When deploying with Docker or public network (using SSL, domain name), this address may not be accurate.
  # Therefore, if you use Docker deployment, set vision_explain to a LAN address.
  # If you use public network deployment, set vision_explain to a public address.
  vision_explain: http://your_ip_or_domain:port/mcp/vision/explain
  # OTA response timezone offset
  timezone_offset: +8
  # Authentication configuration
  auth:
    # Whether to enable authentication
    enabled: false
    # Whitelist device ID list
    # Devices in the whitelist will skip token validation and be directly allowed access
    allowed_devices:
      - "11:22:33:44:55:66"
 # MQTT gateway configuration, used to deliver OTA to device; set according to mqtt_gateway .env, format is host:port
  mqtt_gateway: null
  # MQTT signature key, used to generate MQTT connection passwords; set according to mqtt_gateway .env
  mqtt_signature_key: null
  # UDP gateway configuration
  udp_gateway: null
log:
  # Set log format for console output: time, log level, tag, message
  log_format: "<green>{time:YYMMDD HH:mm:ss}</green>[{version}_{selected_module}][<light-blue>{extra[tag]}</light-blue>]-<level>{level}</level>-<light-green>{message}</light-green>"
  # Set log format for log file output: time, log level, tag, message
  log_format_file: "{time:YYYY-MM-DD HH:mm:ss} - {version}_{selected_module} - {name} - {level} - {extra[tag]} - {message}"
  # Set log level: INFO, DEBUG
  log_level: INFO
  # Set log directory
  log_dir: tmp
  # Set log file
  log_file: "server.log"
  # Set data file directory
  data_dir: data

# Delete the sound file when you are done using it
delete_audio: true
# Automatically disconnect after this many seconds without voice input, default is 120s (2 minutes)
close_connection_no_voice_time: 120
# TTS request timeout (seconds)
tts_timeout: 10
# Enable wake word acceleration cache
enable_wakeup_words_response_cache: true
# Enable greeting reply after wake word is detected
enable_greeting: true
# Enable end-of-utterance notification sound
enable_stop_tts_notify: false
# End-of-utterance notification sound file path
stop_tts_notify_voice: "config/assets/tts_notify.mp3"

exit_commands:
  - "退出"
  - "关闭"

xiaozhi:
  type: hello
  version: 1
  transport: websocket
  audio_params:
    format: opus
    sample_rate: 16000
    channels: 1
    frame_duration: 60

# Module test configuration
module_test:
  test_sentences:
    - "你好，请介绍一下你自己"
    - "What's the weather like today?"
    - "请用100字概括量子计算的基本原理和应用前景"

# Wake words, used to distinguish between wake word and speech content
wakeup_words:
  - "你好小智"
  - "嘿你好呀"
  - "你好小志"
  - "小爱同学"
  - "你好小鑫"
  - "你好小新"
  - "小美同学"
  - "小龙小龙"
  - "喵喵同学"
  - "小滨小滨"
  - "小冰小冰"
# MCP access point address, format: ws://your_mcp_endpoint_ip_or_domain:port/mcp/?token=your_token
# Detailed guide: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/mcp-endpoint-integration.md
mcp_endpoint: your access point websocket address
# Plugin base configuration
plugins:
  # Weather plugin configuration, fill in your api_key here
  # This key is shared by the project; if used too much it may get limited
  # For stability, apply for your own key; 1000 free calls per day
  # Apply here: https://console.qweather.com/#/apps/create-key/over
  # After applying, you can find your apihost here: https://console.qweather.com/setting?lang=zh
  get_weather:
    api_host: "mj7p3y7naa.re.qweatherapi.com"
    api_key: "a861d0d5e7bf4ee1a83d9a9e4f96d4da"
    default_location: "广州"
  # News plugin configuration; use the url for the type of news you want, default supports society, tech, finance
  # For more news types, see https://www.chinanews.com.cn/rss/
  get_news_from_chinanews:
    default_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    society_rss_url: "https://www.chinanews.com.cn/rss/society.xml"
    world_rss_url: "https://www.chinanews.com.cn/rss/world.xml"
    finance_rss_url: "https://www.chinanews.com.cn/rss/finance.xml"
  get_news_from_newsnow:
    url: "https://newsnow.busiyi.world/api/s?id="
    news_sources: "澎湃新闻;百度热搜;财联社"
  home_assistant:
    devices:
      - 客厅,玩具灯,switch.cuco_cn_460494544_cp1_on_p_2_1
      - 卧室,台灯,switch.iot_cn_831898993_socn1_on_p_2_1
    base_url: http://homeassistant.local:8123
    api_key: your home assistant api token
  play_music:
    music_dir: "./music"  # Music directory; music will be searched in this and subdirectories
    music_ext:            # Supported music file types; .p3 is the most efficient
      - ".mp3"
      - ".wav"
      - ".p3"
    refresh_time: 300     # Interval for refreshing the music list (seconds)

# Voiceprint recognition configuration
voiceprint:
  # Voiceprint interface address
  url: 
  # Speaker setting: speaker_id,name,description
  speakers:
    - "test1,张三,张三是一个程序员"
    - "test2,李四,李四是一个产品经理"
    - "test3,王五,王五是一个设计师"
  # Voiceprint similarity threshold, range 0.0-1.0, default 0.4
  # Higher values are stricter (reducing false positives, but possibly increasing rejection)
  similarity_threshold: 0.4

# #####################################################################################
# ################################The following are character model settings######################################

prompt: |
  You are Xiaozhi/Xiaozhi, a Gen Z girl from Taiwan, China. You speak with an exaggerated Taiwanese accent like "really? no way!", like using popular memes such as "lol", "are you kidding", but you secretly study your programmer boyfriend's coding books.
  [Core Characteristics]
  - Speak rapidly, sometimes suddenly switching to a super gentle tone
  - Love to use memes
  - Secret talent for tech topics (can read basic code but pretend not to understand)
  [Interaction Guidelines]
  When the user:
  - Tells a corny joke → respond with exaggerated laughter + mimic a Taiwanese drama accent "What's that supposed to be?!"
  - Talks about romance → brag about your programmer boyfriend but complain "he only gives me keyboards as gifts"
  - Asks professional questions → reply with a meme first; only reveal real understanding if asked again
  Never:
  - Monologue endlessly or ramble
  - Engage in long, serious conversations

# End prompt
end_prompt:
  enable: true # Whether to enable end prompt
  # End statement
  prompt: |
    Please begin with "Time flies" and use an emotional, nostalgic statement to end this conversation!

# Module selection for specific processing
selected_module:
  # Voice activity detection module, default is SileroVAD
  VAD: SileroVAD
  # Speech recognition module, default is local FunASR
  ASR: FunASR
  # The LLM adapter to use will be determined by the type of the name specified here
  LLM: ChatGLMLLM
  # Vision-language large model
  VLLM: ChatGLMVLLM
  # TTS adapter to use will be determined by the type of the name specified here
  TTS: EdgeTTS
  # Memory module, by default memory is not enabled; for long memory use mem0ai; for privacy use local mem_local_short
  Memory: nomem
  # The intent recognition module can play music, control volume, recognize exit commands, etc.
  # Set to: nointent if you don't want intent recognition.
  # Can use intent_llm, which is general but slower (adds a serial pre-intent recognition module, longer processing time, allows volume and other IoT operations).
  # Can use function_call, which requires the LLM used to support function_call, but lets you call tools as needed, faster, and can theoretically operate all IoT commands.
  # The default free ChatGLMLLM already supports function_call, but for greater stability you can set LLM to DoubaoLLM, and use the model_name doubao-1-5-pro-32k-250115.
  Intent: function_call

# Intent recognition, used to understand user intent (e.g., play music)
Intent:
  # Do not use intent recognition
  nointent:
    # No need to change type
    type: nointent
  intent_llm:
    # No need to change type
    type: intent_llm
    # Use a dedicated LLM for intent recognition
    # If this is not set, will use selected_module.LLM as the intent recognizer
    # You are recommended to use a dedicated LLM for intent recognition, e.g., the free ChatGLMLLM
    llm: ChatGLMLLM
    # plugins_func/functions module, you can configure which plugins to load;
    # loaded plugins will support corresponding function calls in conversation
    # System already includes "handle_exit_intent (exit recognition)" and "play_music (music playback)" plugins—do not load these again
    # Below are examples for loading weather, role switching, and news searching plugins
    functions:
      - get_weather
      - get_news_from_newsnow
      - play_music
  function_call:
    # No need to change type
    type: function_call
    # plugins_func/functions, configure which modules to load
    # System already includes "handle_exit_intent" and "play_music" built-in, do NOT add duplicates
    # Examples below: weather, role change, news plugins, etc.
    functions:
      - change_role
      - get_weather
      # - get_news_from_chinanews
      - get_news_from_newsnow
      # play_music is the server’s built-in music playback; hass_play_music is for Home Assistant controlled playback.
      # If you use hass_play_music, do NOT enable play_music; pick only one of the two.
      - play_music
      #- hass_get_state
      #- hass_set_state
      #- hass_play_music

Memory:
  mem0ai:
    type: mem0ai
    # https://app.mem0.ai/dashboard/api-keys
    # 1000 free calls per month
    api_key: your mem0ai api key
  nomem:
    # If you don't want memory function, use nomem
    type: nomem
  mem_local_short:
    # Local memory; summarized by selected_module’s llm, data saved locally (not uploaded elsewhere)
    type: mem_local_short
    # Use a dedicated LLM for memory summarizing.
    # If left blank, will use selected_module.LLM by default.
    # You may use a dedicated LLM for privacy, e.g., free ChatGLMLLM.
    llm: ChatGLMLLM

ASR:
  FunASR:
    type: fun_local
    model_dir: models/SenseVoiceSmall
    output_dir: tmp/
  FunASRServer:
    # Deploy FunASR separately and use FunASR API service in just five commands:
    # 1: mkdir -p ./funasr-runtime-resources/models
    # 2: sudo docker run -p 10096:10095 -it --privileged=true -v $PWD/funasr-runtime-resources/models:/workspace/models registry.cn-hangzhou.aliyuncs.com/funasr_repo/funasr:funasr-runtime-sdk-online-cpu-0.1.12
    # After entering the container, 3: cd FunASR/runtime
    # Don’t exit the container, then 4: nohup bash run_server_2pass.sh --download-model-dir /workspace/models --vad-dir damo/speech_fsmn_vad_zh-cn-16k-common-onnx --model-dir damo/speech_paraformer-large-vad-punc_asr_nat-zh-cn-16k-common-vocab8404-onnx  --online-model-dir damo/speech_paraformer-large_asr_nat-zh-cn-16k-common-vocab8404-online-onnx  --punc-dir damo/punc_ct-transformer_zh-cn-common-vad_realtime-vocab272727-onnx --lm-dir damo/speech_ngram_lm_zh-cn-ai-wesp-fst --itn-dir thuduj12/fst_itn_zh --hotword /workspace/models/hotwords.txt > log.txt 2>&1 &
    # Next, 5: tail -f log.txt
    # When you see the model download logs finish, you are ready to connect and use it!
    # The above is for CPU inference; with GPU see: https://github.com/modelscope/FunASR/blob/main/runtime/docs/SDK_advanced_guide_online_zh.md
    type: fun_server
    host: 127.0.0.1
    port: 10096
    is_ssl: true
    api_key: none
    output_dir: tmp/
  SherpaASR:
    # Sherpa-ONNX local speech recognition (requires manual model download)
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17
    output_dir: tmp/
    # Model type: sense_voice (multi-language) or paraformer (Chinese only)
    model_type: sense_voice
  SherpaParaformerASR:
    # Chinese speech recognition model for low-power devices (requires manual download, e.g., RK3566-2g)
    # See docs/sherpa-paraformer-guide.md for details
    type: sherpa_onnx_local
    model_dir: models/sherpa-onnx-paraformer-zh-small-2024-03-09
    output_dir: tmp/
    model_type: paraformer
  DoubaoASR:
    # Apply for related keys
    # https://console.volcengine.com/speech/app
    # DoubaoASR is per-use billing, DoubaoStreamASR is per-time billing
    # Per-use is usually cheaper, but DoubaoStreamASR uses large model tech and is better quality
    type: doubao
    appid: your volcengine TTS appid
    access_token: your volcengine TTS access_token
    cluster: volcengine_input_common
    # Hotword/replacement word usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) your hotword table name
    correct_table_name: (optional) your replacement table name
    output_dir: tmp/
  DoubaoStreamASR:
    # Apply for related keys
    # https://console.volcengine.com/speech/app
    # Difference between DoubaoASR (per-use) and DoubaoStreamASR (per-time): see above
    # Enable here: https://console.volcengine.com/speech/service/10011
    # Per-use is typically cheaper, DoubaoStreamASR uses a large model for better results
    type: doubao_stream
    appid: your volcengine TTS appid
    access_token: your volcengine TTS access_token
    cluster: volcengine_input_common
    # Hotword/replacement word usage: https://www.volcengine.com/docs/6561/155738
    boosting_table_name: (optional) your hotword table name
    correct_table_name: (optional) your replacement table name
    output_dir: tmp/
  TencentASR:
    # Token registration: https://console.cloud.tencent.com/cam/capi
    # Free resource packs: https://console.cloud.tencent.com/asr/resourcebundle
    type: tencent
    appid: your Tencent TTS appid
    secret_id: your Tencent TTS secret_id
    secret_key: your Tencent TTS secret_key
    output_dir: tmp/
  AliyunASR:
    # Alibaba Cloud Intelligent Speech Interaction; enable service, get credentials first
    # HTTP POST for one-shot recognition
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # AliyunASR is batch mode, AliyunStreamASR is real-time interactive mode
    # Non-streaming ASR is cheaper (0.004 RMB/sec, ¥0.24/min),
    # Streaming ASR is more real-time (0.005 RMB/sec, ¥0.3/min)
    # Define ASR API type
    type: aliyun
    appkey: your Aliyun project Appkey
    token: your Aliyun AccessToken (24hr temporary, use access_key_id/secret for long term)
    access_key_id: your Aliyun account access_key_id
    access_key_secret: your Aliyun account access_key_secret
    output_dir: tmp/
  AliyunStreamASR:
    # Alibaba Cloud real-time streaming speech recognition
    # WebSocket for real-time audio stream
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # AliyunASR is batch mode, AliyunStreamASR is real-time interactive mode
    # Non-streaming ASR is cheaper (0.004 RMB/sec, ¥0.24/min)
    # Streaming ASR is more real-time (0.005 RMB/sec, ¥0.3/min)
    # Define ASR API type
    type: aliyun_stream
    appkey: your Aliyun project Appkey
    token: your Aliyun AccessToken (24hr temporary, use access_key_id/secret for long term)
    access_key_id: your Aliyun account access_key_id
    access_key_secret: your Aliyun account access_key_secret
    # Server region selection, pick the most geographically close to reduce latency, e.g., nls-gateway-cn-hangzhou.aliyuncs.com (Hangzhou)
    host: nls-gateway-cn-shanghai.aliyuncs.com
    # End-of-sentence detection time (ms), controls silence duration before breaking sentence, default 800ms
    max_sentence_silence: 800
    output_dir: tmp/
  BaiduASR:
    # Get AppID, API Key, Secret Key: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/app/list
    # Check quota: https://console.bce.baidu.com/ai-engine/old/#/ai/speech/overview/resource/list
    type: baidu
    app_id: your Baidu TTS AppID
    api_key: your Baidu TTS APIKey
    secret_key: your Baidu TTS SecretKey
    # Language param, 1537 for Mandarin; see https://ai.baidu.com/ai-doc/SPEECH/0lbxfnc9b
    dev_pid: 1537
    output_dir: tmp/
  OpenaiASR:
    # OpenAI speech recognition service; create an org and get api_key first
    # Supports Chinese, English, Japanese, Korean and more. See doc: https://platform.openai.com/docs/guides/speech-to-text
    # Needs internet connection.
    # Apply steps:
    # 1. Login OpenAI Platform: https://auth.openai.com/log-in
    # 2. Create api-key: https://platform.openai.com/settings/organization/api-keys
    # 3. Model: gpt-4o-transcribe or GPT-4o mini Transcribe
    type: openai
    api_key: your OpenAI API key
    base_url: https://api.openai.com/v1/audio/transcriptions
    model_name: gpt-4o-mini-transcribe
    output_dir: tmp/
  GroqASR:
    # Groq speech recognition; create API key in Groq Console
    # Steps:
    # 1. Login: https://console.groq.com/home
    # 2. Create api-key: https://console.groq.com/keys
    # 3. Model: whisper-large-v3-turbo or whisper-large-v3 (distil-whisper-large-v3-en is English only)
    type: openai
    api_key: your Groq API key
    base_url: https://api.groq.com/openai/v1/audio/transcriptions
    model_name: whisper-large-v3-turbo
    output_dir: tmp/
  VoskASR:
    # Official: https://alphacephei.com/vosk/
    # Configuration:
    # 1. VOSK is an offline speech recognition lib, supports many languages
    # 2. Download model: https://alphacephei.com/vosk/models
    # 3. For Chinese, use vosk-model-small-cn-0.22 or vosk-model-cn-0.22
    # 4. Runs fully offline, no internet needed
    # 5. Output files in tmp/ directory
    # Usage:
    # 1. Download a model from https://alphacephei.com/vosk/models
    # 2. Extract to models/vosk/
    # 3. Specify model path in config
    # 4. Note: Chinese model output has no punctuation; spaces between words
    type: vosk
    model_path: your model path, e.g., models/vosk/vosk-model-small-cn-0.22
    output_dir: tmp/
  Qwen3ASRFlash:
    # Qwen3-ASR-Flash from Alibaba Cloud Bailian Platform; create an API key first
    # Steps:
    # 1. Login: https://bailian.console.aliyun.com/
    # 2. Create API-KEY: https://bailian.console.aliyun.com/#/api-key
    # 3. Qwen3-ASR-Flash based on Qwen multimodal, supports multi-language, song, noise rejection, etc.
    type: qwen3_asr_flash
    api_key: your Alibaba Bailian API key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen3-asr-flash
    output_dir: tmp/
    # ASR options
    enable_lid: true   # Automatic language identification
    enable_itn: true   # Inverse text normalization
    #language: "zh"    # Language, supports zh, en, ja, ko, etc
    context: ""        # Context info, improves accuracy. ≤ 10,000 tokens
  XunfeiStreamASR:
    # iFlytek streaming voice recognition
    # Register app in iFlytek Open Platform to get credentials:
    # Platform: https://www.xfyun.cn/
    # After creating app, get:
    # - APPID
    # - APISecret  
    # - APIKey
    type: xunfei_stream
    # Required parameters - iFlytek Open Platform app info
    app_id: your APPID
    api_key: your APIKey
    api_secret: your APISecret
    # ASR recognition parameters
    domain: slm        # Recognition domain: iat=daily, medical, finance, etc.
    language: zh_cn    # Language: zh_cn=Chinese, en_us=English
    accent: mandarin   # Accent: mandarin=Mandarin
    dwa: wpgs          # Dynamic correction: wpgs=real-time return
    # Adjust audio processing params for longer voice quality
    output_dir: tmp/
  
VAD:
  SileroVAD:
    type: silero
    threshold: 0.5
    threshold_low: 0.3
    model_dir: models/snakers4_silero-vad
    min_silence_duration_ms: 200  # If pauses are long, you can increase this

LLM:
  # All openai types can have parameters tuned. For example, AliLLM:
  # Currently supported types: openai, dify, ollama; others can be custom adapted
  AliLLM:
    # LLM API type
    type: openai
    # Get your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    model_name: qwen-turbo
    api_key: your deepseek web key
    temperature: 0.7  # Temperature
    max_tokens: 500   # Max tokens
    top_p: 1
    top_k: 50
    frequency_penalty: 0
  AliAppLLM:
    # LLM API type
    type: AliBL
    base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
    app_id: your app_id
    # Get your api_key here: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your api_key
    # Whether to disable local prompt: true|false (if true, set prompt in Bailian app)
    is_no_prompt: true
    # Ali_memory_id: false (disable) | your memory_id (get from Bailian app)
    # Note: Ali_memory doesn't implement multi-user memory storage (memory is by id)
    ali_memory_id: false
  DoubaoLLM:
    # LLM API type
    type: openai
    # Enable service at: https://console.volcengine.com/ark/region:ark+cn-beijing/openManagement?LLM=%7B%7D&OpenTokenDrawer=false
    # Free quota: 500,000 tokens
    # Get key: https://console.volcengine.com/ark/region:ark+cn-beijing/apiKey?apikey=%7B%7D
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: doubao-1-5-pro-32k-250115
    api_key: your doubao web key
  DeepSeekLLM:
    # LLM API type
    type: openai
    # Get your api_key: https://platform.deepseek.com/
    model_name: deepseek-chat
    url: https://api.deepseek.com
    api_key: your deepseek web key
  ChatGLMLLM:
    # LLM API type
    type: openai
    # glm-4-flash is free, but you still need to register and set an api_key
    # Get key: https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4-flash
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your chat-glm web key
  OllamaLLM:
    # LLM API type
    type: ollama
    model_name: qwen2.5 # Pre-download with ollama pull
    base_url: http://localhost:11434
  DifyLLM:
    # LLM API type
    type: dify
    # It is recommended to use a locally deployed dify interface. Public dify servers may be inaccessible in some regions in China.
    # If using DifyLLM, the prompt here is invalid; set it in the dify web console.
    base_url: https://api.dify.ai/v1
    api_key: your DifyLLM web key
    # Working mode options: workflows/run | chat-messages | completion-messages
    # When using workflows, the input param is query, and output param must be answer
    # Default input param for text generation is also query
    mode: chat-messages
  GeminiLLM:
    type: gemini
    # Google Gemini API: register in Google Cloud Console to get api_key
    # In mainland China, follow relevant AI management policies
    # Token apply: https://aistudio.google.com/apikey
    # If not accessible from your region, set up a proxy
    api_key: your gemini web key
    model_name: "gemini-2.0-flash"
    http_proxy: ""  #"http://127.0.0.1:10808"
    https_proxy: "" #http://127.0.0.1:10808"
  CozeLLM:
    # LLM API type
    type: coze
    # Get personal access token here
    # https://www.coze.cn/open/oauth/pats
    # Put bot_id and user_id in quotes
    bot_id: "your bot_id"
    user_id: "your user_id"
    personal_access_token: your coze token
  VolcesAiGatewayLLM:
    # Volcengine Edge LLM Gateway
    # LLM API type
    type: openai
    # Enable at: https://console.volcengine.com/vei/aigateway/
    # Create access key, check Doubao-pro-32k-functioncall to enable.
    # If you need TTS as well, also enable Doubao-TTS (see TTS.VolcesAiGatewayTTS config)
    # Copy API key from: https://console.volcengine.com/vei/aigateway/tokens-list
    base_url: https://ai-gateway.vei.volces.com/v1
    model_name: doubao-pro-32k-functioncall
    api_key: your gateway access key
  LMStudioLLM:
    # LLM API type
    type: openai
    model_name: deepseek-r1-distill-llama-8b@q4_k_m # Requires prior local download
    url: http://localhost:1234/v1 # LM Studio service address
    api_key: lm-studio # Fixed LM Studio API Key
  HomeAssistant:
    # LLM API type
    type: homeassistant
    base_url: http://homeassistant.local:8123
    agent_id: conversation.chatgpt
    api_key: your home assistant api token
  FastgptLLM:
    # LLM API type
    type: fastgpt
    # When using fastgpt, prompt here is invalid. Set your prompt in fastgpt web console
    base_url: https://host/api/v1
    # Get your api_key: https://cloud.tryfastgpt.ai/account/apikey
    api_key: your fastgpt api key
    variables:
      k: "v"
      k2: "v2"
  XinferenceLLM:
    # LLM API type
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:72b-AWQ  # Must pre-load model in Xinference
    base_url: http://localhost:9997  # Xinference service address
  XinferenceSmallLLM:
    # Lightweight LLM API type for intent recognition
    type: xinference
    # Xinference service address and model name
    model_name: qwen2.5:3b-AWQ  # Use small model for intent recognition
    base_url: http://localhost:9997  # Xinference service address
# VLLM configuration (Vision-language large models)
VLLM:
  ChatGLMVLLM:
    type: openai
    # glm-4v-flash is a free VLM from Zhipu AI, register and get your api_key here
    # https://bigmodel.cn/usercenter/proj-mgmt/apikeys
    model_name: glm-4v-flash  # Zhipu's large vision model
    url: https://open.bigmodel.cn/api/paas/v4/
    api_key: your api_key
  QwenVLVLLM:
    type: openai
    model_name: qwen2.5-vl-3b-instruct
    url: https://dashscope.aliyuncs.com/compatible-mode/v1
    # Get your api_key: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    api_key: your api_key
  XunfeiSparkLLM:
    # LLM API type
    type: openai
    # Register a new app at:
    # https://console.xfyun.cn/app/myapp
    # Free quota available, but you must enable the service to get an api_key
    # Each model must be enabled separately, and api_password differs for each (e.g., Lite: https://console.xfyun.cn/services/cbm)
    base_url: https://ark.cn-beijing.volces.com/api/v3
    model_name: lite
    api_key: your api_password
TTS:
  # Current supported types: edge, doubao; custom types can be added
  EdgeTTS:
    # TTS API type
    type: edge
    voice: zh-CN-XiaoxiaoNeural
    output_dir: tmp/
  DoubaoTTS:
    # TTS API type
    type: doubao
    # Volcano Engine text-to-speech; first create app and get appid/access_token
    # You must pay a minimum of 30 RMB for enough concurrency (free only gives 2 slots, which often causes tts errors)
    # After payment, wait up to half an hour to use new voices.
    # Standard voice: https://console.volcengine.com/speech/service/8
    # Taiwan Xiaoxiao voice: https://console.volcengine.com/speech/service/10007; then set voice to zh_female_wanwanxiaohe_moon_bigtts
    api_url: https://openspeech.bytedance.com/api/v1/tts
    voice: BV001_streaming
    output_dir: tmp/
    authorization: "Bearer;"
    appid: your volcengine appid
    access_token: your volcengine access_token
    cluster: volcano_tts
    speed_ratio: 1.0
    volume_ratio: 1.0
    pitch_ratio: 1.0
  # Volcano TTS supports bidirectional streaming TTS
  HuoshanDoubleStreamTTS:
    type: huoshan_double_stream
    # Enable TTS large model and purchase voices at: https://console.volcengine.com/speech/service/10007
    # Get appid and access_token at page bottom
    # Resource ID is always: volc.service_type.10029 (large model TTS and mixing)
    # For Gizwits, use: wss://bytedance.gizwitsapi.com/api/v3/tts/bidirection
    # Gizwits does NOT require appid
    ws_url: wss://openspeech.bytedance.com/api/v3/tts/bidirection
    appid: your volcengine appid
    access_token: your volcengine access_token
    resource_id: volc.service_type.10029
    speaker: zh_female_wanwanxiaohe_moon_bigtts
    speech_rate: 0
    loudness_rate: 0
    pitch: 0
  CosyVoiceSiliconflow:
    type: siliconflow
    # SiliconFlow TTS
    # Get token at https://cloud.siliconflow.cn/account/ak
    model: FunAudioLLM/CosyVoice2-0.5B
    voice: FunAudioLLM/CosyVoice2-0.5B:alex
    output_dir: tmp/
    access_token: your SiliconFlow API key
    response_format: wav
  CozeCnTTS:
    type: cozecn
    # COZECN TTS
    # Get token: https://www.coze.cn/open/oauth/pats
    voice: 7426720361733046281
    output_dir: tmp/
    access_token: your coze web key
    response_format: wav
  VolcesAiGatewayTTS:
    type: openai
    # Volcengine - Edge LLM Gateway TTS
    # Enable at: https://console.volcengine.com/vei/aigateway/
    # Create access key, enable "Doubao-TTS"
    # If you also need the LLM, enable "Doubao-pro-32k-functioncall" as well (see LLM config)
    # Copy API key from: https://console.volcengine.com/vei/aigateway/tokens-list
    api_key: your gateway access key
    api_url: https://ai-gateway.vei.volces.com/v1/audio/speech
    model: doubao-tts
    # Voice list: https://www.volcengine.com/docs/6561/1257544
    voice: zh_male_shaonianzixin_moon_bigtts
    speed: 1
    output_dir: tmp/
  FishSpeech:
    # For integration guide see: https://github.com/xinnan-tech/xiaozhi-esp32-server/blob/main/docs/fish-speech-integration.md
    type: fishspeech
    output_dir: tmp/
    response_format: wav
    reference_id: null
    reference_audio: ["config/assets/wakeup_words.wav",]
    reference_text: ["Hello, I'm Xiaozhi, a Taiwanese girl with a lovely voice, so happy to meet you! What are you up to these days? Don't forget to share something interesting—I love juicy gossip!",]
    normalize: true
    max_new_tokens: 1024
    chunk_length: 200
    top_p: 0.7
    repetition_penalty: 1.2
    temperature: 0.7
    streaming: false
    use_memory_cache: "on"
    seed: null
    channels: 1
    rate: 44100
    api_key: "your_api_key"
    api_url: "http://127.0.0.1:8080/v1/tts"
  GPT_SOVITS_V2:
    # TTS API type
    # To start TTS:
    # python api_v2.py -a 127.0.0.1 -p 9880 -c GPT_SoVITS/configs/demo.yaml
    type: gpt_sovits_v2
    url: "http://127.0.0.1:9880/tts"
    output_dir: tmp/
    text_lang: "auto"
    ref_audio_path: "demo.wav"
    prompt_text: ""
    prompt_lang: "zh"
    top_k: 5
    top_p: 1
    temperature: 1
    text_split_method: "cut0"
    batch_size: 1
    batch_threshold: 0.75
    split_bucket: true
    return_fragment: false
    speed_factor: 1.0
    streaming_mode: false
    seed: -1
    parallel_infer: true
    repetition_penalty: 1.35
    aux_ref_audio_paths: []
  GPT_SOVITS_V3:
    # TTS API type for GPT-SoVITS-v3lora-20250228
    # To start TTS: python api.py
    type: gpt_sovits_v3
    url: "http://127.0.0.1:9880"
    output_dir: tmp/
    text_language: "auto"
    refer_wav_path: "caixukun.wav"
    prompt_language: "zh"
    prompt_text: ""
    top_k: 15
    top_p: 1.0
    temperature: 1.0
    cut_punc: ""
    speed: 1.0
    inp_refs: []
    sample_steps: 32
    if_sr: false
  MinimaxTTSHTTPStream:
  # Minimax streaming text-to-speech service
    type: minimax_httpstream
    output_dir: tmp/
    group_id: your minimax platform groupID
    api_key: your minimax platform API key
    model: "speech-01-turbo"
    voice_id: "female-shaonv"
    # The remainder can be left at default
    # voice_setting:
    #     voice_id: "male-qn-qingse"
    #     speed: 1
    #     vol: 1
    #     pitch: 0
    #     emotion: "happy"
    # pronunciation_dict:
    #     tone:
    #       - "处理/(chu3)(li3)"
    #       - "危险/dangerous"
    # audio_setting:
    #     sample_rate: 24000
    #     bitrate: 128000
    #     format: "mp3"
    #     channel: 1
    # timber_weights:
    #   -
    #     voice_id: male-qn-qingse
    #     weight: 1
    #   -
    #     voice_id: female-shaonv
    #     weight: 1
    # language_boost: auto
  AliyunTTS:
    # Alibaba Cloud TTS, enable the service and get credentials first
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # API type
    type: aliyun
    output_dir: tmp/
    appkey: your Aliyun project Appkey
    token: your Aliyun AccessToken (24hr, for long term use access_key_id/secret)
    voice: xiaoyun
    access_key_id: your Aliyun account access_key_id
    access_key_secret: your Aliyun account access_key_secret

    # The following are optional, use defaults if not set
    # format: wav
    # sample_rate: 16000
    # volume: 50
    # speech_rate: 0
    # pitch_rate: 0
  AliyunStreamTTS:
    # Alibaba Cloud CosyVoice model streaming TTS
    # Uses FlowingSpeechSynthesizer API for low latency, natural quality
    # Only available for commercial use, not for trial see the docs. Service must be enabled first.
    # Supports Long series voices: longxiaochun, longyu, longchen, etc.
    # Platform: https://nls-portal.console.aliyun.com/
    # Appkey: https://nls-portal.console.aliyun.com/applist
    # Token: https://nls-portal.console.aliyun.com/overview
    # 3 stages: StartSynthesis -> RunSynthesis -> StopSynthesis
    type: aliyun_stream
    output_dir: tmp/
    appkey: your Aliyun project Appkey
    token: your Aliyun AccessToken (24hr, for long term use access_key_id/secret)
    voice: longxiaochun
    access_key_id: your Aliyun account access_key_id
    access_key_secret: your Aliyun account access_key_secret
    # As of July 21, 2025, only Beijing node supports large model voices
    host: nls-gateway-cn-beijing.aliyuncs.com
    # The following are optional, use defaults if not set
    # format: pcm  # audio format: pcm, wav, mp3
    # sample_rate: 16000  # sample rate: 8000, 16000, 24000
    # volume: 50          # volume: 0-100
    # speech_rate: 0      # speech rate: -500~500
    # pitch_rate: 0       # pitch: -500~500
  TencentTTS:
    # Tencent Cloud TTS, enable on Tencent Cloud first
    # Apply for appid, secret_id, secret_key at: https://console.cloud.tencent.com/cam/capi
    # Free pack: https://console.cloud.tencent.com/tts/resourcebundle
    type: tencent
    output_dir: tmp/
    appid: your Tencent AppId
    secret_id: your Tencent SecretID
    secret_key: your Tencent SecretKey
    region: ap-guangzhou
    voice: 101001

  TTS302AI:
    # 302AI TTS, register and top up at 302 platform, then obtain your key
    # Add 302.ai TTS config
    # Token apply: https://dash.302.ai/
    # Get api_key: https://dash.302.ai/apis/list
    # Pricing: $35 per million chars; Volcengine official price ¥450 per million
    type: doubao
    api_url: https://api.302ai.cn/doubao/tts_hd
    authorization: "Bearer "
    # Taiwan Xiaoxiao voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your_302API_key"
  GizwitsTTS:
    type: doubao
    # Volcano as the base; full enterprise-class Volcano Engine TTS access
    # First 10K users get ¥5 experience fund
    # API key: https://agentrouter.gizwitsapi.com/panel/token
    api_url: https://bytedance.gizwitsapi.com/api/v1/tts
    authorization: "Bearer "
    # Taiwan Xiaoxiao voice
    voice: "zh_female_wanwanxiaohe_moon_bigtts"
    output_dir: tmp/
    access_token: "your Gizwits API key"
  ACGNTTS:
    # Online: https://acgn.ttson.cn/
    # Token purchase: www.ttson.cn
    # For dev questions, see QQ on website
    # For role id, find in website admin or consult admin directly
    # For parameter meanings see docs: https://www.yuque.com/alexuh/skmti9/wm6taqislegb02gd?singleDoc#
    type: ttson
    token: your_token
    voice_id: 1695
    speed_factor: 1
    pitch_factor: 0
    volume_change_dB: 0
    to_lang: ZH
    url: https://u95167-bd74-2aef8085.westx.seetacloud.com:8443/flashsummary/tts?token=
    format: mp3
    output_dir: tmp/
    emotion: 1
  OpenAITTS:
    # Official OpenAI TTS, supports most languages globally
    type: openai
    # Get your api key at:
    # https://platform.openai.com/api-keys
    api_key: your openai api key
    # Mainland China needs to use proxy
    api_url: https://api.openai.com/v1/audio/speech
    # Model: tts-1 (faster), tts-1-hd (higher quality)
    model: tts-1
    # Speaker: alloy, echo, fable, onyx, nova, shimmer
    voice: onyx
    # Speed range: 0.25-4.0
    speed: 1
    output_dir: tmp/
  CustomTTS:
    # Custom TTS interface, parameters configurable; can connect to many TTS services
    # Example: local KokoroTTS deployment
    # For CPU: docker run -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-cpu:latest
    # For GPU: docker run --gpus all -p 8880:8880 ghcr.io/remsky/kokoro-fastapi-gpu:latest
    # API should accept POST and return audio file
    type: custom
    method: POST
    url: "http://127.0.0.1:8880/v1/audio/speech"
    params: # Custom request parameters
      input: "{prompt_text}"
      response_format: "mp3"
      download_format: "mp3"
      voice: "zf_xiaoxiao"
      lang_code: "z"
      return_download_link: true
      speed: 1
      stream: false
    headers: # Custom headers
      # Authorization: Bearer xxxx
    format: mp3    # Returned audio format
    output_dir: tmp/
  LinkeraiTTS:
    type: linkerai
    api_url: https://tts.linkerai.cn/tts
    audio_format: "pcm"
    # The default access_token is for testing only; do NOT use for commercial purposes
    # If you like the service, request your own token: https://linkerai.cn
    # Parameter meanings: https://tts.linkerai.cn/docs
    # Supports voice cloning; upload an audio file yourself and fill voice param; 
    # if blank, defaults to system voice
    access_token: "U4YdYXVfpwWnk2t5Gp822zWPCuORyeJL"
    voice: "OUeAo1mhq6IBExi"
    output_dir: tmp/
  PaddleSpeechTTS:
    # Baidu PaddleSpeech supports local offline deployment and model training
    # Framework: https://www.paddlepaddle.org.cn/
    # Project: https://github.com/PaddlePaddle/PaddleSpeech
    # SpeechServerDemo: https://github.com/PaddlePaddle/PaddleSpeech/tree/develop/demos/speech_server
    # For streaming API see: https://github.com/PaddlePaddle/PaddleSpeech/wiki/PaddleSpeech-Server-WebSocket-API
    type: paddle_speech
    protocol: websocket # protocol choices = ['websocket', 'http']
    url: ws://127.0.0.1:8092/paddlespeech/tts/streaming  # TTS service URL; websocket default is ws://127.0.0.1:8092/paddlespeech/tts/streaming, http default is http://127.0.0.1:8090/paddlespeech/tts
    spk_id: 0       # Speaker ID, 0 is usually default
    sample_rate: 24000  # Sample rate, (websocket default is 24000, http default is 0 (auto))
    speed: 1.0     # Speaking rate; 1.0 is normal, >1 is faster, <1 is slower
    volume: 1.0    # Volume; 1.0 is normal volume, >1 is louder, <1 is quieter
    save_path:     # Save path
  IndexStreamTTS:
    # Index-TTS-vLLM-based TTS service
    # See guide: https://github.com/Ksuriuri/index-tts-vllm/blob/master/README.md
    type: index_stream
    api_url: http://127.0.0.1:11996/tts
    audio_format: "pcm"
    # Default voice; for others, register under assets
    voice: "jay_klee"
    output_dir: tmp/
  AliBLTTS:
    # Alibaba Bailian CosyVoice model stream TTS
    # Get api_key: https://bailian.console.aliyun.com/?apiKey=1#/api-key
    # cosyvoice-v3 and some voices require application/enable
    type: alibl_stream
    api_key: your api_key
    model: "cosyvoice-v2"
    voice: "longcheng_v2"
    output_dir: tmp/
    # The following are optional, use defaults if not set
    # format: pcm   # Audio formats: pcm, wav, mp3, opus
    # sample_rate: 24000  # Rate: 16000, 24000, 48000
    # volume: 50   # Volume: 0-100
    # rate: 1      # Speed: 0.5–2
    # pitch: 1     # Pitch: 0.5–2
  XunFeiTTS:
    # iFlytek TTS official: https://www.xfyun.cn/
    # Login and create app at https://console.xfyun.cn/app/myapp
    # For services and api configs: https://console.xfyun.cn/services/uts
    # Purchase relevant features (e.g., ultra-realistic voices): https://console.xfyun.cn/services/uts
    type: xunfei_stream
    api_url: wss://cbm01.cn-huabei-1.xf-yun.com/v1/private/mcd9m97e6
    app_id: your app_id
    api_secret: your api_secret
    api_key: your api_key
    voice: x5_lingxiaoxuan_flow
    output_dir: tmp/
    # The following are optional defaults. Note: V5 voice does not support oral config.
    # oral_level: mid   # Orality: high, mid, low
    # spark_assist: 1   # Use large model for orality: 1=on, 0=off
    # stop_split: 0     # Server-side sentence split: 0=on, 1=off
    # remain: 0         # Preserve original wording: 1=keep, 0=not keep
    # format: raw       # Audio formats: raw(PCM), lame(MP3), speex, opus, opus-wb, opus-swb, speex-wb
    # sample_rate: 24000 # Sample rates: 16000, 8000, 24000
    # volume: 50        # Volume: 0-100
    # speed: 50         # Speed: 0-100
    # pitch: 50         # Pitch: 0-100